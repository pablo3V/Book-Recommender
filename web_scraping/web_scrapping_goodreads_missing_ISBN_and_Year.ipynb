{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10006dda-2e1f-459b-9cd9-0f1c33b77a54",
   "metadata": {},
   "source": [
    "# Web Scraping in Goodreads\n",
    "\n",
    "This notebook is designed to obtain the **ISBN** and **Year** of books from the Goodreads website. The books for which we need to collect this data are those listed in the datasets available [here](https://github.com/zygmuntz/goodbooks-10k). Some of these books have these features missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c4379-8e5e-47fe-9d6e-7146ef891199",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9086083c-def5-4f07-8578-fa893538eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd               # pandas is used for data manipulation and analysis, providing data structures like DataFrames.\n",
    "import numpy as np                # numpy is used for numerical operations on large, multi-dimensional arrays and matrices.\n",
    "import requests                   # Library used for making HTTP requests.\n",
    "from bs4 import BeautifulSoup     # Library for parsing HTML and XML documents.\n",
    "from tqdm import tqdm             # To include a progress bar in the loop.\n",
    "import concurrent.futures         # To make multiple http requests simultaneously.\n",
    "import time                       # time is used for time-related functions\n",
    "import re                         # re provides regular expression matching operations in strings.\n",
    "import json                       # json is used for parsing and generating JSON (JavaScript Object Notation) data.\n",
    "from datetime import datetime     # datetime is used for manipulating dates and times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab905866-4221-4301-9290-1c7eb74aa8a6",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31ca3b-38fa-4452-b8ab-9fa3a94dd464",
   "metadata": {},
   "source": [
    "Here we define functions that will be used in a loop to make HTTP requests to Goodreads. These functions search for the publication year and ISBN of a book with a given GoodreadsID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b73cb48-c42d-468e-912a-d0c1bcea9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The header is for the simultaneous requests to work.\n",
    "# It seems that Goodreads blocks the requests made through a script.\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "def get_book_editions(book_workid):\n",
    "    url = f\"https://www.goodreads.com/work/editions/{book_workid}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        #print(f\"Error fetching the page: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    editions = soup.find_all('div', class_='editionData')\n",
    "\n",
    "    book_editions = []\n",
    "    for edition in editions:\n",
    "        edition_info = {}\n",
    "        title_tag = edition.find('a', class_='bookTitle')\n",
    "        if title_tag:\n",
    "            edition_info['title'] = title_tag.text.strip()\n",
    "            edition_info['link'] = \"https://www.goodreads.com\" + title_tag['href']\n",
    "    \n",
    "        if edition_info:\n",
    "            book_editions.append(edition_info)\n",
    "\n",
    "    return book_editions\n",
    "\n",
    "\n",
    "def get_missing_data(edition_url):\n",
    "    response = requests.get(edition_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        #print(f\"Error fetching the page: {response.status_code}\")\n",
    "        return None, None\n",
    "\n",
    "    #match = re.search(r'{\"__typename\":\"BookDetails\",\"[^}]*\"isbn\":\"[^\"]*\"', response.text)\n",
    "    match = re.search(r'{\"__typename\":\"BookDetails\",\"[^}]*\"language\":{\"__typename\":\"Language\",\"name\":\"[^\"]*\"}', response.text)\n",
    "    \n",
    "    if not match:\n",
    "        #print(\"No JSON data found\")\n",
    "        return None, None\n",
    "\n",
    "    json_data = match.group(0) + '}'  # We close the JSON with '\"}'\n",
    "\n",
    "    # Convert the JSON fragment into a dictionary\n",
    "    try:\n",
    "        book_details = json.loads(json_data)\n",
    "    except json.JSONDecodeError:\n",
    "        #print(\"Error decoding JSON\")\n",
    "        return None, None\n",
    "\n",
    "    isbn = book_details.get('isbn')\n",
    "    publication_time = book_details.get('publicationTime')\n",
    "    language = book_details.get('language').get('name')\n",
    "\n",
    "    # Convert publication time into a friendly format\n",
    "    if publication_time:\n",
    "        year = datetime.utcfromtimestamp(publication_time / 1000).strftime('%Y-%m-%d')[0:4]\n",
    "    else:\n",
    "        year = None\n",
    "\n",
    "    if language != 'English':\n",
    "        return None, None\n",
    "\n",
    "    #print(book_details)\n",
    "    return isbn, year \n",
    "\n",
    "\n",
    "def get_data(workid):\n",
    "    editions = get_book_editions(workid)\n",
    "    for edition in editions:\n",
    "        isbn, year = get_missing_data(edition['link'])\n",
    "        if isbn:\n",
    "            return isbn, year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6594f2-a3df-4851-a943-8ebc1a587375",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Here we load the books dataset from which we can get the Goodreads ID of the books for which we want to find the ISBNs and Years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4d4ca5d-e208-4e8d-a061-3e7f1824688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"../data_preprocessed/books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94df015-68ef-4b21-bac5-509a228b0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   book_id                    10000 non-null  int64  \n",
      " 1   goodreads_book_id          10000 non-null  int64  \n",
      " 2   best_book_id               10000 non-null  int64  \n",
      " 3   work_id                    10000 non-null  int64  \n",
      " 4   books_count                10000 non-null  int64  \n",
      " 5   isbn                       9300 non-null   object \n",
      " 6   isbn13                     9415 non-null   float64\n",
      " 7   authors                    10000 non-null  object \n",
      " 8   original_publication_year  9979 non-null   float64\n",
      " 9   original_title             9415 non-null   object \n",
      " 10  title                      10000 non-null  object \n",
      " 11  language_code              8916 non-null   object \n",
      " 12  average_rating             10000 non-null  float64\n",
      " 13  ratings_count              10000 non-null  int64  \n",
      " 14  work_ratings_count         10000 non-null  int64  \n",
      " 15  work_text_reviews_count    10000 non-null  int64  \n",
      " 16  ratings_1                  10000 non-null  int64  \n",
      " 17  ratings_2                  10000 non-null  int64  \n",
      " 18  ratings_3                  10000 non-null  int64  \n",
      " 19  ratings_4                  10000 non-null  int64  \n",
      " 20  ratings_5                  10000 non-null  int64  \n",
      " 21  image_url                  10000 non-null  object \n",
      " 22  small_image_url            10000 non-null  object \n",
      "dtypes: float64(3), int64(13), object(7)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "books.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a7609d-dc85-4583-810d-a1c825221f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_books = books[books['isbn'].isnull() | books['original_publication_year'].isnull()].index\n",
    "books_missing_data = books.loc[missing_data_books].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2721a0-5414-49b5-a41e-07851e0dab61",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06985e-aa3a-48c0-a9d4-59b62a26ab06",
   "metadata": {},
   "source": [
    "We divide the array of GoodreadsIDs into a number of intervals determined by `divs`. This approach allows us to iterate over these intervals and further within the GoodreadsIDs inside each interval, specifying how many intervals we compile each time. This method gives us control over the web scraping process, as simply iterating over the entire array of GoodreadsIDs, i.e., `books['goodreads_book_id']`, could potentially lead to various issues. Additionally, every time an interval is completed, we store the results in a CSV file to prevent any potential loss of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a88c776c-c067-410b-baa5-25aa86fddf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00%|██████████████████████████████████████| 359/359 [00:00<06:28,  1.08s/it]\n",
      "100.00%|██████████████████████████████████████| 359/359 [00:00<07:19,  1.23s/it]\n"
     ]
    }
   ],
   "source": [
    "total = len(books_missing_data['work_id']) # total number of ISBNs\n",
    "divs = 2 # number of intervals in which we divide the isbns\n",
    "step = int(total / divs) # length of each interval\n",
    "ranges = [range(step*i - step, step*i) for i in range(1,divs+1)] # an array with the intervals\n",
    "\n",
    "key_i = 0 # number of the interval at which we start the for loop\n",
    "key_f = 2 # number of the interval at which we stop the for loop\n",
    "\n",
    "#progress_bar = tqdm(total=(key_f-key_i), bar_format='{percentage:.2f}%|{bar}| {n_fmt}/{total_fmt} [{remaining}<{elapsed}, {rate_fmt}]')\n",
    "\n",
    "for i in range(key_i, key_f): \n",
    "    \n",
    "    workids = books_missing_data['work_id'][ranges[i]] # ISBNs of the interval i\n",
    "    \n",
    "    progress_bar = tqdm(total=step, bar_format='{percentage:.2f}%|{bar}| {n_fmt}/{total_fmt} [{remaining}<{elapsed}, {rate_fmt}]')\n",
    "    \n",
    "    data_dict = {\n",
    "        'WorkID':[],\n",
    "        'ISBN':[],\n",
    "        'Year':[]\n",
    "    }\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "        future_to_workid = {executor.submit(get_data, workid): workid for workid in workids}\n",
    "        for future in concurrent.futures.as_completed(future_to_workid):\n",
    "            workid = future_to_workid[future]\n",
    "            try:\n",
    "                isbn_workid, year_workid = future.result()\n",
    "                data_dict['WorkID'].append(workid)\n",
    "                data_dict['ISBN'].append(isbn_workid)\n",
    "                data_dict['Year'].append(year_workid)\n",
    "            except Exception as e:\n",
    "                data_dict['WorkID'].append(workid)\n",
    "                data_dict['ISBN'].append(np.nan)\n",
    "                data_dict['Year'].append(np.nan)\n",
    "            progress_bar.update(1)\n",
    "           \n",
    "    progress_bar.close()\n",
    "\n",
    "    new_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    try:\n",
    "        existing_df = pd.read_csv(\"books_data_missing.txt\", sep=\"\\t\")\n",
    "    except FileNotFoundError:\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "    combined_df.to_csv(\"books_data_missing.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    #progress_bar.update(1) \n",
    "    \n",
    "#progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
