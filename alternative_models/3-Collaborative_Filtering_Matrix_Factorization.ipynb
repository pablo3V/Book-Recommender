{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23a9ffba-2ea4-4671-84b0-8d92a41406d0",
   "metadata": {},
   "source": [
    "# Book Recommendation System\n",
    "\n",
    "# Part III: Collaborative Filtering - Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c432d3-28b1-4994-a023-d3feac0d6cd7",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bdb44a9-f3dd-42b8-a1b8-53ce15a4d82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 14:15:35.869298: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-06 14:15:35.872934: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-06 14:15:35.923566: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-06 14:15:36.713969: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/pabloev/anaconda3/envs/data_science/lib/python3.11/site-packages/dask/dataframe/__init__.py:49: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd               # pandas is used for data manipulation and analysis, providing data structures like DataFrames.\n",
    "import numpy as np                # numpy is used for numerical operations on large, multi-dimensional arrays and matrices.\n",
    "\n",
    "from scipy.sparse import csr_matrix                             # csr_matrix is used for creating compressed sparse row matrices, which are efficient for arithmetic and matrix operations on sparse data.\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# IPython's display module is used to display images within Jupyter Notebooks.\n",
    "from IPython.display import Markdown, display, Image  \n",
    "from IPython.display import clear_output\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be169b5-6ebc-4ba6-8425-28901ef3e536",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8bd38da-619a-4290-ba07-a6d3afe01d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"data/Books_cleaned.csv\").drop('Unnamed: 0', axis = 1)\n",
    "\n",
    "ratings_files = [f'data/Ratings_cleaned_part_{i}.csv' for i in range(1,6+1)]\n",
    "ratings_dfs = [pd.read_csv(file) for file in ratings_files]\n",
    "ratings = pd.concat(ratings_dfs, ignore_index=True).drop('Unnamed: 0', axis = 1)\n",
    "del ratings_files, ratings_dfs\n",
    "\n",
    "books_genres = pd.read_csv(\"data/Books_genres_cleaned.csv\").drop('Unnamed: 0', axis = 1)\n",
    "books_genres_list = pd.read_csv(\"data/Books_genres_list_cleaned.csv\").drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b4252-96d6-4289-bf3a-cd21873e470d",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43bf71-0071-4642-bb9f-a19bd69ced17",
   "metadata": {},
   "source": [
    "### Step 1. Preparing the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b297a39-ae2c-45ad-af92-00c6157f7d2c",
   "metadata": {},
   "source": [
    "The goal of a collaborative filtering recommender system is to generate two vectors: For each user, a 'parameter vector' that embodies the movie tastes of a user. For each item, a feature vector of the same size which embodies some description of the item. The dot product of the two vectors plus the bias term should produce an estimate of the rating the user might give to that item. This approach is known as matrix factorization.\n",
    "\n",
    "Matrix factorization is a powerful approach to collaborative filtering, designed to work with sparse datasets like user ratings for books. Unlike user-based or item-based collaborative filtering, which rely on explicit similarities, matrix factorization discovers hidden patterns that connect users and items.\n",
    "\n",
    "Imagine you’re trying to choose a new book to read. Instead of looking for users with similar reading habits or books that share obvious traits, matrix factorization uncovers abstract dimensions, like a user’s preference for complex plots or a book’s appeal in a particular genre. These hidden factors help make personalized and accurate recommendations.\n",
    "\n",
    "Here’s how it works:\n",
    "\n",
    "1. Representing Users and Books as Latent Features:\n",
    "\n",
    "    - Matrix factorization maps both users and books into a latent feature space. Each user is represented by a vector of preferences, and each book is represented by a vector of attributes in this space.\n",
    "\n",
    "    - For instance, a user who consistently rates certain books highly might have a strong preference for hidden factors captured by the model, such as a common theme, tone, or writing style that those books share. The model does not know these characteristics explicitly but infers them from the user’s ratings.\n",
    "\n",
    "2. Predicting Ratings:\n",
    "\n",
    "    - The model predicts how much a user would like a book by computing the dot product of their feature vector and the book’s feature vector, plus a bias term for both the user and the book.\n",
    "\n",
    "3. Training the Model:\n",
    "\n",
    "    - The system adjusts the user and book vectors by minimizing the error between predicted and actual ratings in the training data. This optimization process ensures that the model learns meaningful latent factors that best explain the observed ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2168d0de-f30e-4fe4-9f13-a07b790f5810",
   "metadata": {},
   "source": [
    "The existing ratings are meant to be provided in the form of a matrix, $Y$. This matrix is a $n_i \\times n_u$ matrix, where $n_i$ is the number of items and $n_u$ is the number of users. Then, $Y[i, j]$ represents the rating that the user $j$ has given to the item $i$.\n",
    "\n",
    "But first, if I want the algorithm to make recommendation for myself, I have to enter my own ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea24e44-0682-4e2a-91b9-00fa8411d275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BookID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>213</td>\n",
       "      <td>The Metamorphosis</td>\n",
       "      <td>Franz Kafka, Stanley Corngold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2979</th>\n",
       "      <td>3020</td>\n",
       "      <td>The Metamorphosis and Other Stories</td>\n",
       "      <td>Franz Kafka, Jason Baker, Donna Freed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6555</th>\n",
       "      <td>6664</td>\n",
       "      <td>The Metamorphosis, In the Penal Colony, and Ot...</td>\n",
       "      <td>Franz Kafka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BookID                                              Title  \\\n",
       "211      213                                  The Metamorphosis   \n",
       "2979    3020                The Metamorphosis and Other Stories   \n",
       "6555    6664  The Metamorphosis, In the Penal Colony, and Ot...   \n",
       "\n",
       "                                    Authors  \n",
       "211           Franz Kafka, Stanley Corngold  \n",
       "2979  Franz Kafka, Jason Baker, Donna Freed  \n",
       "6555                            Franz Kafka  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To look for the BookIDs\n",
    "books[books['Title'].str.contains('metamorphosis', case=False)][['BookID','Title','Authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd52dec8-aa3f-4cbd-84af-2cf95986a7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rated 4 for The Metamorphosis\n",
      "Rated 4 for The Way of Shadows (Night Angel, #1)\n",
      "Rated 4 for Shadow's Edge (Night Angel, #2)\n",
      "Rated 4 for Beyond the Shadows (Night Angel, #3)\n",
      "Rated 4 for The Hobbit\n",
      "Rated 5 for The Fellowship of the Ring (The Lord of the Rings, #1)\n",
      "Rated 5 for The Two Towers (The Lord of the Rings, #2)\n",
      "Rated 5 for The Return of the King (The Lord of the Rings, #3)\n",
      "Rated 5 for The Final Empire (Mistborn, #1)\n",
      "Rated 5 for The Well of Ascension (Mistborn, #2)\n",
      "Rated 5 for The Hero of Ages (Mistborn, #3)\n",
      "Rated 4 for The Alloy of Law (Mistborn, #4)\n",
      "Rated 4 for Shadows of Self (Mistborn, #5)\n",
      "Rated 4 for The Bands of Mourning (Mistborn, #6)\n",
      "Rated 5 for Warbreaker (Warbreaker, #1)\n",
      "Rated 5 for The Way of Kings (The Stormlight Archive, #1)\n",
      "Rated 5 for The Catcher in the Rye\n",
      "Rated 5 for The Name of the Wind (The Kingkiller Chronicle, #1)\n",
      "Rated 4 for The Color of Magic (Discworld, #1; Rincewind #1)\n",
      "Rated 4 for The Light Fantastic (Discworld, #2; Rincewind #2)\n",
      "Rated 3 for Equal Rites (Discworld, #3; Witches #1)\n",
      "Rated 4 for Sourcery (Discworld, #5; Rincewind #3)\n",
      "Rated 3 for A Court of Thorns and Roses (A Court of Thorns and Roses, #1)\n",
      "Rated 4 for A Court of Mist and Fury (A Court of Thorns and Roses, #2)\n",
      "Rated 4 for A Court of Wings and Ruin (A Court of Thorns and Roses, #3)\n",
      "Rated 4 for Chronicle of a Death Foretold\n",
      "Rated 5 for The Three-Body Problem (Remembrance of Earth’s Past, #1)\n",
      "Rated 5 for The Dark Forest (Remembrance of Earth’s Past, #2)\n",
      "Rated 5 for Foundation (Foundation #1)\n",
      "Rated 5 for Foundation and Empire (Foundation #2)\n",
      "Rated 5 for Second Foundation (Foundation #3)\n",
      "Rated 4 for The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1)\n",
      "Rated 3 for Flatland: A Romance of Many Dimensions\n"
     ]
    }
   ],
   "source": [
    "# In order to have recommendations for the target user, me in this case, \n",
    "# I add my ratings and user information in the datasets\n",
    "my_ratings = {\n",
    "    'UserID': [19960808]*33,\n",
    "    'BookID': [213,   # The Metamorphosis\n",
    "               859,   # The Way of Shadows\n",
    "               1412,  # Shadow's Edge\n",
    "               1429,  # Beyond the Shadows\n",
    "               7,     # The Hobbit\t\n",
    "               19,    # The Fellowship of the Ring\n",
    "               155,   # The Two Towers\n",
    "               161,   # The Return of the King\n",
    "               389,   # The Final Empire\n",
    "               565,   # The Well of Ascension\n",
    "               603,   # The Hero of Ages \n",
    "               1200,  # The Alloy of Law\n",
    "               2792,  # Shadows of Self\n",
    "               3341,  # The Bands of Mourning\n",
    "               1665,  # Warbreaker\n",
    "               562,   # The Way of Kings\n",
    "               8,     # The Catcher in the Rye\n",
    "               192,   # The Name of the Wind\n",
    "               429,   # The Color of Magic\n",
    "               1343,  # The Light Fantastic\n",
    "               1089,  # Equal Rites\n",
    "               2109,  # Sourcery\n",
    "               842,   # A Court of Thorns and Roses\n",
    "               1308,  # A Court of Mist and Fury\n",
    "               7373,  # A Court of Wings and Ruin\n",
    "               1239,  # Chronicle of a Death Foretold\n",
    "               2676,  # The Three-Body Problem\n",
    "               7120,  # The Dark Forest\n",
    "               276,   # Foundation\n",
    "               789,   # Foundation and Empire\n",
    "               890,   # Second Foundation \n",
    "               54,    # The Hitchhiker's Guide to the Galaxy\n",
    "               2931,  # Flatland: A Romance of Many Dimensions\n",
    "              ],\n",
    "    'Rating': [4, # The Metamorphosis\n",
    "               4, # The Way of Shadows\n",
    "               4, # Shadow's Edge\n",
    "               4, # Beyond the Shadows\n",
    "               4, # The Hobbit\t\n",
    "               5, # The Fellowship of the Ring\n",
    "               5, # The Two Towers\n",
    "               5, # The Return of the King\n",
    "               5, # The Final Empire\n",
    "               5, # The Well of Ascension\n",
    "               5, # The Hero of Ages \n",
    "               4, # The Alloy of Law\n",
    "               4, # Shadows of Self\n",
    "               4, # The Bands of Mourning\n",
    "               5, # Warbreaker\n",
    "               5, # The Way of Kings\n",
    "               5, # The Catcher in the Rye\n",
    "               5, # The Name of the Wind\n",
    "               4, # The Color of Magic\n",
    "               4, # The Light Fantastic\n",
    "               3, # Equal Rites\n",
    "               4, # Sourcery\n",
    "               3, # A Court of Thorns and Roses\n",
    "               4, # A Court of Mist and Fury\n",
    "               4, # A Court of Wings and Ruin\n",
    "               4, # Chronicle of a Death Foretold\n",
    "               5, # The Three-Body Problem\n",
    "               5, # The Dark Forest\n",
    "               5, # Foundation\n",
    "               5, # Foundation and Empire\n",
    "               5, # Second Foundation \n",
    "               4, # The Hitchhiker's Guide to the Galaxy\n",
    "               3, # Flatland: A Romance of Many Dimensions              \n",
    "              ]\n",
    "}\n",
    "\n",
    "my_ratings_df = pd.DataFrame(my_ratings)\n",
    "ratings = pd.concat([ratings, my_ratings_df], ignore_index=True)\n",
    "\n",
    "for index, row in my_ratings_df.iterrows():\n",
    "    userid = row['UserID']\n",
    "    bookid = row['BookID']\n",
    "    book_title = books[books['BookID'] == bookid]['Title'].values[0]\n",
    "    rating = row['Rating']\n",
    "    print(f'Rated {rating} for {book_title}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47a23e-701f-42c4-be9e-6828ae547320",
   "metadata": {},
   "source": [
    "Now that I have the target user ratings, we select the users that have rated, at least, one of the books the target user has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a681aa8-fb88-4795-8b45-9ab17df61682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of users:  53346\n",
      "Users with, at least, 1 coincidence:  34590\n",
      "Users with, at least, 10 coincidence:  1276\n"
     ]
    }
   ],
   "source": [
    "target_UserID = 19960808\n",
    "\n",
    "# Original number of users\n",
    "print('Original number of users: ', len(ratings['UserID'].unique()))\n",
    "\n",
    "# Books rated by the target user\n",
    "target_books = ratings[ratings['UserID'] == target_UserID].BookID.values\n",
    "\n",
    "# Users who have rated at least 1 of the items rated by the current user\n",
    "selected_users_1 = ratings[ratings['BookID'].isin(target_books)]\n",
    "selected_users_1 = pd.DataFrame(selected_users_1.groupby('UserID').size(), columns=['Coincidences']).sort_values(by='Coincidences', ascending=False).reset_index()\n",
    "\n",
    "# There are 34590 users with at least one coincidence\n",
    "number_of_users_1 = selected_users_1.shape[0]\n",
    "print('Users with, at least, 1 coincidence: ', number_of_users_1)\n",
    "\n",
    "# In this case with so many users available for recommendations, we can keep just those with at least 10 coincidences\n",
    "selected_users_10 = selected_users_1[selected_users_1['Coincidences'] >= 10]\n",
    "\n",
    "# Now, we have 1279 available users\n",
    "number_of_users_10 = selected_users_10.shape[0]\n",
    "print('Users with, at least, 10 coincidence: ', number_of_users_10)\n",
    "\n",
    "# Ratings of the selected users\n",
    "selected_ratings = ratings[ratings['UserID'].isin(selected_users_10.UserID.values)].reset_index().drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5cd404-5a66-4427-897f-67c0435969ec",
   "metadata": {},
   "source": [
    "I can now construct the $Y$ matrix with the ratings. To do so, I will use a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb23075b-957d-49a2-ab93-8b8c0b97864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the csr_matrix: 8837576\n",
      "Number of non-zero elements in the csr_matrix: 163709\n"
     ]
    }
   ],
   "source": [
    "# Get the values of UserUDs and BookIDs to assign indices\n",
    "unique_users = selected_ratings['UserID'].unique()\n",
    "unique_books = selected_ratings['BookID'].unique()\n",
    "\n",
    "# Create a dictionary to map unique IDs to indices\n",
    "user_to_index = {user_id: index for index, user_id in enumerate(unique_users)}\n",
    "book_to_index = {book_id: index for index, book_id in enumerate(unique_books)}\n",
    "\n",
    "# Map the UserIDs and BookIDs to their respective indices\n",
    "user_indices = selected_ratings['UserID'].map(user_to_index)\n",
    "book_indices = selected_ratings['BookID'].map(book_to_index)\n",
    "\n",
    "# Create the CSR matrix\n",
    "ratings_csr_matrix = csr_matrix(\n",
    "    (selected_ratings['Rating'], (book_indices, user_indices)),\n",
    "    shape=(len(unique_books), len(unique_users))\n",
    ")\n",
    "\n",
    "print('Total size of the csr_matrix:', ratings_csr_matrix.shape[0] * ratings_csr_matrix.shape[1])\n",
    "print('Number of non-zero elements in the csr_matrix:', ratings_csr_matrix.count_nonzero())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30631e7-3b0b-4e3c-b480-0b6863b2d210",
   "metadata": {},
   "source": [
    "This reflects the importance of using a sparse matrix here. Notice how much larger the total size of the array is compared to the number of non-zero elements, which is just the number of ratings of the selected users. Sparse arrays/matrices allow us to represent these objects without explicitly storing all the 0-valued elements. This means that if the transactional data can be loaded into memory, the sparse array will fit in memory as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1485f08f-6a6a-4e1d-b3ef-2b768fcc11bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the row 130845 of the ratings dataframe we have:\n",
      "\t UserID = 52183\n",
      "\t BookID = 3111\n",
      "\t Rating = 4\n",
      "\n",
      "Using book_indices and user_indices one can map the IDs to the indices in the sparse matrix:\n",
      "\t Book index = 2682 (row index)\n",
      "\t User index = 1221 (column index)\n",
      "\n",
      "Then, using these indices:\n",
      "\t CSR_Matrix[2682, 1221] = 4 (rating)\n",
      "\n",
      "One can also go from the indices in the CSR Matrix to the UserID and BookID values:\n",
      "\t UserID = unique_users[1221] = 52183\n",
      "\t BookID = unique_books[2682] = 3111\n"
     ]
    }
   ],
   "source": [
    "# Example to understand how the csr matrix is created\n",
    "i = 130845 # row of the ratings dataframe\n",
    "\n",
    "print(f'In the row {i} of the ratings dataframe we have:')\n",
    "print(f'\\t UserID = {selected_ratings.iloc[i].UserID}')\n",
    "print(f'\\t BookID = {selected_ratings.iloc[i].BookID}')\n",
    "print(f'\\t Rating = {selected_ratings.iloc[i].Rating}\\n')\n",
    "\n",
    "print(f'Using book_indices and user_indices one can map the IDs to the indices in the sparse matrix:')\n",
    "print(f'\\t Book index = {book_indices[i]} (row index)')\n",
    "print(f'\\t User index = {user_indices[i]} (column index)\\n')\n",
    "\n",
    "print(f'Then, using these indices:')\n",
    "print(f'\\t CSR_Matrix[{book_indices[i]}, {user_indices[i]}] = {ratings_csr_matrix[book_indices[i], user_indices[i]]} (rating)\\n')\n",
    "\n",
    "print('One can also go from the indices in the CSR Matrix to the UserID and BookID values:')\n",
    "print(f'\\t UserID = unique_users[{user_indices[i]}] = {unique_users[user_indices[i]]}')\n",
    "print(f'\\t BookID = unique_books[{book_indices[i]}] = {unique_books[book_indices[i]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291915f9-588c-40f1-b3be-cf3334775f0d",
   "metadata": {},
   "source": [
    "Now, let's define and normalize the matrices $Y$ and $R$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b480d89-426f-482d-9bc0-5986334a4590",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                                                                              #\n",
    "#                              normalizeRatings                                #\n",
    "#                                                                              #\n",
    "#  Preprocess data by subtracting the mean rating for every movie (every row)  #\n",
    "#  so that each movie has a rating of 0 on average. Unrated moves then have a  #\n",
    "#  mean rating (0).                                                            #\n",
    "#  Only include real ratings R(i,j) = 1.                                       #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "\n",
    "def normalizeRatings(Y, R):\n",
    "    Ymean = np.sum(Y ,axis=1) / np.sum(R, axis=1) # We are safe from having a 0 in the denominator because all the books have, at least, a rating\n",
    "    Ynorm = Y - R.multiply(Ymean[:, 0])\n",
    "    return(Ynorm, Ymean)\n",
    "\n",
    "# # Check Ymean is well calculated\n",
    "# Ymean = np.sum(Y ,axis=1) / np.sum(R, axis=1)\n",
    "# equal = True\n",
    "# for i in range(len(Ymean)):\n",
    "#     mean = selected_ratings[selected_ratings['BookID'] == unique_books[i]]['Rating'].mean()\n",
    "#     if Ymean[i, 0] != mean:\n",
    "#         equal = False\n",
    "#         break\n",
    "# equal\n",
    "\n",
    "# Check the normalization is well calculated\n",
    "# Ynorm[i, j] (if != 0) should be equal rating_ij - book_i_mean, where\n",
    "# book_id = unique_books[i]\n",
    "# user_id = unique_users[j]\n",
    "# book_i_mean = selected_ratings[selected_ratings['BookID'] == book_id]['Rating'].mean()\n",
    "# rating_ij = ratings[(ratings['BookID'] == book_id) & (ratings['UserID'] == user_id)]['Rating'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc36e2e-23ff-41e6-8850-c5761882cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ratings_csr_matrix.copy()\n",
    "\n",
    "R = ratings_csr_matrix.copy()\n",
    "R.data = (R.data != 0).astype(int) # The R matrix returns a 0 if the rating of a given user to a given book exists and 1 otherwise\n",
    "\n",
    "# Normalize Y\n",
    "Ynorm, Ymean = normalizeRatings(Y, R)\n",
    "\n",
    "del ratings_csr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4584189-8a1a-45e6-96db-7948d18fc844",
   "metadata": {},
   "source": [
    "Let's now prepare to train the model. Initialize the parameters and select the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76fc551-cc62-4b56-8efb-06ffc664ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Values\n",
    "num_books, num_users = Y.shape\n",
    "num_features = 100\n",
    "\n",
    "# Set Initial Parameters (W, X), use tf.Variable to track these variables\n",
    "tf.random.set_seed(1234) # for consistent results\n",
    "W = tf.Variable(tf.random.normal((num_users,  num_features),dtype=tf.float64),  name='W')\n",
    "X = tf.Variable(tf.random.normal((num_books,  num_features),dtype=tf.float64),  name='X')\n",
    "b = tf.Variable(tf.random.normal((1,          num_users),   dtype=tf.float64),  name='b')\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3048cb3-e6e3-44ed-aa40-5e0f6bc6e14d",
   "metadata": {},
   "source": [
    "Let's now train the collaborative filtering model. This will learn the parameters $\\mathbf{X}$, $\\mathbf{W}$, and $\\mathbf{b}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75f4a5-0f7a-4f89-b024-a1ceca5edda9",
   "metadata": {},
   "source": [
    "The operations involved in learning $w$, $b$, and $x$ simultaneously do not fall into the typical 'layers' offered in the TensorFlow neural network package.  Consequently, the flow used in Course 2: Model, Compile(), Fit(), Predict(), are not directly applicable. Instead, we can use a custom training loop.\n",
    "\n",
    "Recall from earlier labs the steps of gradient descent.\n",
    "- repeat until convergence:\n",
    "    - compute forward pass\n",
    "    - compute the derivatives of the loss relative to parameters\n",
    "    - update the parameters using the learning rate and the computed derivatives \n",
    "    \n",
    "TensorFlow has the marvelous capability of calculating the derivatives for you. This is shown below. Within the `tf.GradientTape()` section, operations on Tensorflow Variables are tracked. When `tape.gradient()` is later called, it will return the gradient of the loss relative to the tracked variables. The gradients can then be applied to the parameters using an optimizer. \n",
    "This is a very brief introduction to a useful feature of TensorFlow and other machine learning frameworks. Further information can be found by investigating \"custom training loops\" within the framework of interest.\n",
    "    \n",
    "But first, we have to construct the cost function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3adb0e23-f3df-4d7f-a09a-2e341d1de22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#                                                                              #\n",
    "#                               cofi_cost_func                                 #\n",
    "#                                                                              #\n",
    "#  Returns the cost for the content-based filtering.                           #\n",
    "#  Vectorized for speed. Uses tensorflow operations to be compatible with      #\n",
    "#  custom training loop.                                                       #\n",
    "#  Arguments:                                                                  #\n",
    "#   X (ndarray (num_movies,num_features)): matrix of item features             #\n",
    "#   W (ndarray (num_users,num_features)) : matrix of user parameters           #\n",
    "#   b (ndarray (1, num_users)            : vector of user parameters           #\n",
    "#   Y (matrix (num_movies,num_users)     : matrix of user ratings of books     #\n",
    "#   R (matrix (num_movies,num_users)    : matrix, where R(i, j) = 1 if the     #\n",
    "#                                       i-th movies was rated by the j-th user #\n",
    "#   lambda_ (float): regularization parameter                                  #\n",
    "#                                                                              #\n",
    "################################################################################\n",
    "\n",
    "def cofi_cost_func(X, W, b, Y, R, lambda_):\n",
    "    # Convert the sparse matrices Y and R into TensorFlow tensors\n",
    "    Y_dense = tf.convert_to_tensor(Y.toarray(), dtype=tf.float64) \n",
    "    R_dense = tf.convert_to_tensor(R.toarray(), dtype=tf.float64) \n",
    "\n",
    "    # Compute the difference between prediction and target\n",
    "    prediction = tf.linalg.matmul(X, tf.transpose(W)) + b  # X * W^T + b\n",
    "    error = (prediction - Y_dense) * R_dense  # Apply the R mask for the non-zero positions\n",
    "\n",
    "    # Cálculo del costo (función de pérdida)\n",
    "    J = 0.5 * tf.reduce_sum(tf.square(error))  # Sum the squared errors\n",
    "    J += (lambda_ / 2) * (tf.reduce_sum(tf.square(X)) + tf.reduce_sum(tf.square(W)))  # Regularization\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2116f8b7-5134-4393-8ca9-782339c6b2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at iteration 0: 8743673.9\n",
      "Training loss at iteration 20: 304267.2\n",
      "Training loss at iteration 40: 111410.0\n",
      "Training loss at iteration 60: 58196.5\n",
      "Training loss at iteration 80: 36952.9\n",
      "Training loss at iteration 100: 26438.9\n",
      "Training loss at iteration 120: 20449.1\n",
      "Training loss at iteration 140: 16729.9\n",
      "Training loss at iteration 160: 14290.1\n",
      "Training loss at iteration 180: 12623.7\n",
      "Training loss at iteration 200: 11447.2\n",
      "Training loss at iteration 220: 10592.9\n",
      "Training loss at iteration 240: 9957.3\n",
      "Training loss at iteration 260: 9474.0\n",
      "Training loss at iteration 280: 9099.2\n",
      "Training loss at iteration 300: 8803.3\n",
      "Training loss at iteration 320: 8565.8\n",
      "Training loss at iteration 340: 8372.2\n",
      "Training loss at iteration 360: 8212.0\n",
      "Training loss at iteration 380: 8077.8\n",
      "Training loss at iteration 400: 7963.8\n",
      "Training loss at iteration 420: 7865.9\n",
      "Training loss at iteration 440: 7781.0\n",
      "Training loss at iteration 460: 7706.8\n",
      "Training loss at iteration 480: 7641.5\n",
      "Training loss at iteration 500: 7583.6\n",
      "Training loss at iteration 520: 7531.9\n",
      "Training loss at iteration 540: 7485.7\n",
      "Training loss at iteration 560: 7444.1\n",
      "Training loss at iteration 580: 7406.4\n",
      "Training loss at iteration 600: 7372.3\n",
      "Training loss at iteration 620: 7341.1\n",
      "Training loss at iteration 640: 7312.6\n",
      "Training loss at iteration 660: 7286.5\n",
      "Training loss at iteration 680: 7262.3\n",
      "Training loss at iteration 700: 7240.1\n",
      "Training loss at iteration 720: 7219.5\n",
      "Training loss at iteration 740: 7200.3\n",
      "Training loss at iteration 760: 7182.5\n",
      "Training loss at iteration 780: 7166.0\n",
      "Training loss at iteration 800: 7150.6\n",
      "Training loss at iteration 820: 7136.1\n",
      "Training loss at iteration 840: 7122.7\n",
      "Training loss at iteration 860: 7110.0\n",
      "Training loss at iteration 880: 7098.2\n",
      "Training loss at iteration 900: 7087.1\n",
      "Training loss at iteration 920: 7076.7\n",
      "Training loss at iteration 940: 7066.9\n",
      "Training loss at iteration 960: 7057.6\n",
      "Training loss at iteration 980: 7048.9\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000\n",
    "lambda_ = 1\n",
    "for iter in range(iterations):\n",
    "    # Use TensorFlow’s GradientTape\n",
    "    # to record the operations used to compute the cost \n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Compute the cost (forward pass included in cost)\n",
    "        cost_value = cofi_cost_func(X, W, b, Ynorm, R, lambda_)\n",
    "\n",
    "    # Use the gradient tape to automatically retrieve\n",
    "    # the gradients of the trainable variables with respect to the loss\n",
    "    grads = tape.gradient( cost_value, [X,W,b] )\n",
    "\n",
    "    # Run one step of gradient descent by updating\n",
    "    # the value of the variables to minimize the loss.\n",
    "    optimizer.apply_gradients( zip(grads, [X,W,b]) )\n",
    "\n",
    "    # Log periodically.\n",
    "    if iter % 20 == 0:\n",
    "        print(f\"Training loss at iteration {iter}: {cost_value:0.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5c1a9a-b66f-42d4-97aa-ea30c152a92e",
   "metadata": {},
   "source": [
    "### Step X. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "8958dfea-8840-497e-85d7-3c8899cd3325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordered indices: [6127 6268 6269 1196 4984 2651 2271 2275 4191 2272]\n"
     ]
    }
   ],
   "source": [
    "# Make the predictions for my user\n",
    "p = np.matmul(X.numpy(), np.transpose(W.numpy())) + b.numpy()\n",
    "\n",
    "# Restore the mean\n",
    "pm = p + Ymean\n",
    "\n",
    "# My user is located at the last index\n",
    "my_predictions = pm[:,-1].flatten()\n",
    "my_predictions = np.array(my_predictions)[0]\n",
    "\n",
    "# Sort predictions\n",
    "ix = np.argsort(my_predictions)[::-1]\n",
    "print(\"Ordered indices:\", ix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0649b91e-06e6-4ec8-8400-ed50be63489f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting rating 5.06 for book: The Choice\n",
      "Predicting rating 5.05 for book: Cry to Heaven\n",
      "Predicting rating 5.05 for book: Belinda\n",
      "Predicting rating 5.05 for book: Selected Poems\n",
      "Predicting rating 5.04 for book: Chronicles, Vol. 1\n",
      "Predicting rating 5.04 for book: The 48 Laws of Power\n",
      "Predicting rating 5.03 for book: Shopaholic Takes Manhattan (Shopaholic, #2)\n",
      "Predicting rating 5.03 for book: Shopaholic & Baby (Shopaholic, #5)\n",
      "Predicting rating 5.03 for book: When the Wind Blows (When the Wind Blows, #1)\n",
      "Predicting rating 5.03 for book: Shopaholic Ties the Knot (Shopaholic, #3)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    book_id = unique_books[ix[i]]\n",
    "    if book_id not in my_ratings_df['BookID'].values:\n",
    "        book_title = books[books['BookID'] == book_id]['Title'].values[0]\n",
    "        print(f'Predicting rating {my_predictions[ix[i]]:0.2f} for book: {book_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e3415993-ba0b-4f18-8f04-8ecb50b7b4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original vs Predicted ratings:\n",
      "\n",
      "Original 4, Predicted 3.95 for The Metamorphosis\n",
      "Original 4, Predicted 3.98 for The Way of Shadows (Night Angel, #1)\n",
      "Original 4, Predicted 4.02 for Shadow's Edge (Night Angel, #2)\n",
      "Original 4, Predicted 4.00 for Beyond the Shadows (Night Angel, #3)\n",
      "Original 4, Predicted 4.01 for The Hobbit\n",
      "Original 5, Predicted 4.97 for The Fellowship of the Ring (The Lord of the Rings, #1)\n",
      "Original 5, Predicted 5.00 for The Two Towers (The Lord of the Rings, #2)\n",
      "Original 5, Predicted 4.98 for The Return of the King (The Lord of the Rings, #3)\n",
      "Original 5, Predicted 4.98 for The Final Empire (Mistborn, #1)\n",
      "Original 5, Predicted 4.98 for The Well of Ascension (Mistborn, #2)\n",
      "Original 5, Predicted 4.99 for The Hero of Ages (Mistborn, #3)\n",
      "Original 4, Predicted 4.00 for The Alloy of Law (Mistborn, #4)\n",
      "Original 4, Predicted 4.06 for Shadows of Self (Mistborn, #5)\n",
      "Original 4, Predicted 4.07 for The Bands of Mourning (Mistborn, #6)\n",
      "Original 5, Predicted 4.91 for Warbreaker (Warbreaker, #1)\n",
      "Original 5, Predicted 5.01 for The Way of Kings (The Stormlight Archive, #1)\n",
      "Original 5, Predicted 4.92 for The Catcher in the Rye\n",
      "Original 5, Predicted 5.00 for The Name of the Wind (The Kingkiller Chronicle, #1)\n",
      "Original 4, Predicted 3.98 for The Color of Magic (Discworld, #1; Rincewind #1)\n",
      "Original 4, Predicted 3.97 for The Light Fantastic (Discworld, #2; Rincewind #2)\n",
      "Original 3, Predicted 3.07 for Equal Rites (Discworld, #3; Witches #1)\n",
      "Original 4, Predicted 3.97 for Sourcery (Discworld, #5; Rincewind #3)\n",
      "Original 3, Predicted 3.08 for A Court of Thorns and Roses (A Court of Thorns and Roses, #1)\n",
      "Original 4, Predicted 4.11 for A Court of Mist and Fury (A Court of Thorns and Roses, #2)\n",
      "Original 4, Predicted 4.33 for A Court of Wings and Ruin (A Court of Thorns and Roses, #3)\n",
      "Original 4, Predicted 3.94 for Chronicle of a Death Foretold\n",
      "Original 5, Predicted 4.81 for The Three-Body Problem (Remembrance of Earth’s Past, #1)\n",
      "Original 5, Predicted 4.90 for The Dark Forest (Remembrance of Earth’s Past, #2)\n",
      "Original 5, Predicted 4.96 for Foundation (Foundation #1)\n",
      "Original 5, Predicted 4.95 for Foundation and Empire (Foundation #2)\n",
      "Original 5, Predicted 4.94 for Second Foundation (Foundation #3)\n",
      "Original 4, Predicted 4.03 for The Hitchhiker's Guide to the Galaxy (Hitchhiker's Guide to the Galaxy, #1)\n",
      "Original 3, Predicted 3.15 for Flatland: A Romance of Many Dimensions\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nOriginal vs Predicted ratings:\\n')\n",
    "my_book_ids = my_ratings_df['BookID'].values\n",
    "for i in range(len(my_book_ids)):\n",
    "    book_id = my_book_ids[i]\n",
    "    index_p = unique_books.tolist().index(book_id)\n",
    "    book_title = books[books['BookID'] == book_id]['Title'].values[0]\n",
    "\n",
    "    my_rating = my_ratings_df[my_ratings_df['BookID'] == book_id]['Rating'].values[0]\n",
    "    print(f'Original {my_rating}, Predicted {my_predictions[index_p]:0.2f} for {book_title}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2f7c3-b3f7-40a8-93f8-d5515e20048c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
