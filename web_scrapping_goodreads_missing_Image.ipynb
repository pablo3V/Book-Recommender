{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10006dda-2e1f-459b-9cd9-0f1c33b77a54",
   "metadata": {},
   "source": [
    "# Web Scraping in Goodreads\n",
    "\n",
    "This notebook is to get the **Image_url** of all these books that have this feature missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c4379-8e5e-47fe-9d6e-7146ef891199",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9086083c-def5-4f07-8578-fa893538eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests # Library used for making HTTP requests\n",
    "from bs4 import BeautifulSoup # Library for parsing HTML and XML documents\n",
    "from tqdm import tqdm # To include a progress bar in the loop\n",
    "import concurrent.futures # To make multiple http requests simultaneously\n",
    "import time # For time-related functions\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import Image # to display images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab905866-4221-4301-9290-1c7eb74aa8a6",
   "metadata": {},
   "source": [
    "### Useful Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31ca3b-38fa-4452-b8ab-9fa3a94dd464",
   "metadata": {},
   "source": [
    "Here we define a function that will be used in the loop to make an HTTP request to Goodreads. Then, the year of publication of the book with a given ISBN is searched for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b73cb48-c42d-468e-912a-d0c1bcea9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is in order for the request to work.\n",
    "# It seems that Goodreads blocks the requests made through a script.\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "def get_book_editions(book_workid):\n",
    "    url = f\"https://www.goodreads.com/work/editions/{book_workid}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        #print(f\"Error fetching the page: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    editions = soup.find_all('div', class_='editionData')\n",
    "\n",
    "    book_editions = []\n",
    "    for edition in editions:\n",
    "        edition_info = {}\n",
    "        title_tag = edition.find('a', class_='bookTitle')\n",
    "        if title_tag:\n",
    "            edition_info['title'] = title_tag.text.strip()\n",
    "            edition_info['link'] = \"https://www.goodreads.com\" + title_tag['href']\n",
    "    \n",
    "        if edition_info:\n",
    "            book_editions.append(edition_info)\n",
    "\n",
    "    return book_editions\n",
    "\n",
    "\n",
    "def is_image_url_accessible(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_missing_data(edition_url):\n",
    "    response = requests.get(edition_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        #print(f\"Error fetching the page: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    script_tag = soup.find('script', type='application/ld+json') # The image's url is here\n",
    "\n",
    "    if not script_tag:\n",
    "        print(\"No JSON-LD script found\")\n",
    "        return None\n",
    "\n",
    "    # Convert the JSON fragment into a dictionary\n",
    "    try:\n",
    "        book_details = json.loads(script_tag.string)\n",
    "    except json.JSONDecodeError:\n",
    "        #print(\"Error decoding JSON\")\n",
    "        return None\n",
    "\n",
    "    image_url = book_details.get('image')\n",
    "\n",
    "    if is_image_url_accessible(image_url) == False:\n",
    "        return None\n",
    "\n",
    "    return image_url\n",
    "\n",
    "\n",
    "def get_data(workid):\n",
    "    editions = get_book_editions(workid)\n",
    "    for edition in editions:\n",
    "        image_url = get_missing_data(edition['link'])\n",
    "        if image_url:\n",
    "            return image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6594f2-a3df-4851-a943-8ebc1a587375",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Here we load the books dataset from which we can get the ISBN of the books for which we want to find the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c4d4ca5d-e208-4e8d-a061-3e7f1824688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"archive/books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e94df015-68ef-4b21-bac5-509a228b0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in books[image_url]: 3331\n"
     ]
    }
   ],
   "source": [
    "columns = ['image_url']\n",
    "for column in columns:\n",
    "    num_duplicates = books[books[column].duplicated()].shape[0]\n",
    "    print(f'Number of duplicates in books[{column}]: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6032a9d7-f85b-41d3-a9f0-242795986f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png'} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_images = books[books['image_url'].duplicated()]['image_url'].values\n",
    "print(set(duplicated_images), '\\n')\n",
    "# The duplicated images correspond all of them to the same picture and it is actually a missing image:\n",
    "url = duplicated_images[0]\n",
    "Image(url=url, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db6930f2-6978-40a2-a2d7-122f47239c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_missing_data = books[books['image_url'] == url].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2721a0-5414-49b5-a41e-07851e0dab61",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06985e-aa3a-48c0-a9d4-59b62a26ab06",
   "metadata": {},
   "source": [
    "What we do is divide the array of work_ids into a number of intervals determined by `divs`. This approach allows us to iterate over these intervals, and further within the work_ids inside each interval, specifying how many intervals we compile each time. This method gives us control over the web scraping, as simply iterating over the entire array of work_ids, i.e., `books['work_id']`, could potentially lead to various issues. Additionally, every time an interval is completed, we store the results in a CSV file to prevent any potential loss of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a88c776c-c067-410b-baa5-25aa86fddf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:47,  1.11it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:06,  1.78it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:28,  1.35it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:02,  1.91it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:02,  1.92it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:20,  1.49it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<02:48,  1.42s/it]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:07,  1.77it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:38,  1.21it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:49,  1.09it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:01,  1.95it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:26,  1.37it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:01,  1.94it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:43,  1.15it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:41,  1.18it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<02:47,  1.41s/it]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:44,  1.14it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:01,  1.95it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:00,  1.95it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:56,  1.02it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:23,  1.42it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:43,  1.15it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:06,  1.78it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:51,  1.06it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<00:58,  2.03it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:02,  1.90it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:26,  1.37it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:39,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "total = len(books_missing_data['work_id']) # total number of ISBNs\n",
    "divs = 28 # number of intervals in which we divide the isbns\n",
    "step = int(total / divs) # length of each interval\n",
    "ranges = [range(step*i - step, step*i) for i in range(1,divs+1)] # an array with the intervals\n",
    "\n",
    "key_i = 0 # number of the interval at which we start the for loop\n",
    "key_f = 28 # number of the interval at which we stop the for loop\n",
    "\n",
    "#progress_bar = tqdm(total=(key_f-key_i), bar_format='{percentage:.2f}%|{bar}| {n_fmt}/{total_fmt} [{remaining}<{elapsed}, {rate_fmt}]')\n",
    "\n",
    "for i in range(key_i, key_f): \n",
    "    \n",
    "    workids = books_missing_data['work_id'][ranges[i]] # WorkIDs of the interval i\n",
    "    \n",
    "    progress_bar = tqdm(total=step, bar_format='{percentage:.2f}%|{bar}| {n_fmt}/{total_fmt} [{remaining}<{elapsed}, {rate_fmt}]')\n",
    "    \n",
    "    data_dict = {\n",
    "        'WorkID':[],\n",
    "        'Image_url':[]\n",
    "    }\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "        future_to_workid = {executor.submit(get_data, workid): workid for workid in workids}\n",
    "        for future in concurrent.futures.as_completed(future_to_workid):\n",
    "            workid = future_to_workid[future]\n",
    "            try:\n",
    "                image_workid = future.result()\n",
    "                data_dict['WorkID'].append(workid)\n",
    "                data_dict['Image_url'].append(image_workid)\n",
    "            except Exception as e:\n",
    "                data_dict['WorkID'].append(workid)\n",
    "                data_dict['Image_url'].append(np.nan)\n",
    "            progress_bar.update(1)\n",
    "           \n",
    "    progress_bar.close()\n",
    "\n",
    "    new_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    try:\n",
    "        existing_df = pd.read_csv(\"books_image_missing.txt\", sep=\"\\t\")\n",
    "    except FileNotFoundError:\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "    combined_df.to_csv(\"books_image_missing.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    #progress_bar.update(1) \n",
    "    \n",
    "#progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88a338de-15f9-44d8-893d-a95d383689cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WorkID</th>\n",
       "      <th>Image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1180927</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1766737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4551869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2247074</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>3125926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1244564</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>25704</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>968512</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>3898716</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>1075398</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>3158544</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>2399497</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>894503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2164503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>621375</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1612568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>2758130</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>3077944</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>54726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>56119</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>3111256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>25833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>2100943</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>21987573</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>3259531</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>9295</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>13897390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>1254615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>308671</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>605176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>6164528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>575600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>65618</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>6642936</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>3107820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>2114967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1629048</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WorkID Image_url\n",
       "117    1180927       NaN\n",
       "118    1766737       NaN\n",
       "142    4551869       NaN\n",
       "170    2247074       NaN\n",
       "237    3125926       NaN\n",
       "356    1244564       NaN\n",
       "427      25704       NaN\n",
       "713     968512       NaN\n",
       "754    3898716       NaN\n",
       "832    1075398       NaN\n",
       "951    3158544       NaN\n",
       "1070   2399497       NaN\n",
       "1189    894503       NaN\n",
       "1427   2164503       NaN\n",
       "1663    621375       NaN\n",
       "1665   1612568       NaN\n",
       "1784   2758130       NaN\n",
       "1902   3077944       NaN\n",
       "1903     54726       NaN\n",
       "1929     56119       NaN\n",
       "2021   3111256       NaN\n",
       "2022     25833       NaN\n",
       "2379   2100943       NaN\n",
       "2411  21987573       NaN\n",
       "2442   3259531       NaN\n",
       "2498      9295       NaN\n",
       "2617  13897390       NaN\n",
       "2736   1254615       NaN\n",
       "2756    308671       NaN\n",
       "2855    605176       NaN\n",
       "2901   6164528       NaN\n",
       "2903    575600       NaN\n",
       "2945     65618       NaN\n",
       "3045   6642936       NaN\n",
       "3212   3107820       NaN\n",
       "3330   2114967      None\n",
       "3331   1629048      None"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d934e8c5-1e0d-4ef7-9dce-79767d2b638b",
   "metadata": {},
   "source": [
    "1. ThreadPoolExecutor: concurrent.futures.ThreadPoolExecutor() creates a pool of threads that can be used to execute multiple calls to the process_isbn function simultaneously. Each thread in the pool will execute a specific task.\n",
    "\n",
    "2. executor.submit(): executor.submit(process_isbn, isbn) submits a task to the thread pool for execution. The submit function takes two arguments: the function to be executed (process_isbn) and the arguments to be passed to that function (isbn). It returns a Future object that represents the future result of the function call.\n",
    "\n",
    "3. {executor.submit(process_isbn, isbn): isbn for isbn in isbns}: This is a dictionary comprehension that creates a dictionary where the keys are Future objects returned by executor.submit() and the values are the corresponding ISBNs. This is used to keep track of which ISBN corresponds to which Future.\n",
    "\n",
    "4. concurrent.futures.as_completed(): This function takes an iterable of Future objects and returns an iterator that yields Future as they are completed. It waits until each Future is completed and then returns the completed Future. In this case, we are passing the future_to_isbn dictionary that contains all the Future objects we have created earlier.\n",
    "\n",
    "5. for future in concurrent.futures.as_completed(future_to_isbn):: We iterate over the iterator returned by as_completed(). As the Future objects are completed, the loop iterates over them in the order they are completed.\n",
    "\n",
    "6. isbn = future_to_isbn[future]: Since we are keeping track of which ISBN corresponds to which Future in our future_to_isbn dictionary, we can use the current Future to find its corresponding ISBN.\n",
    "\n",
    "7. future.result(): future.result() returns the result of the function call associated with the Future. If the function call has not yet finished, result() will block until it is completed. In this case, we are getting the result (i.e., the genre of the book) and storing it in the genres_isbn variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
