{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10006dda-2e1f-459b-9cd9-0f1c33b77a54",
   "metadata": {},
   "source": [
    "# Web Scraping in Goodreads\n",
    "\n",
    "This notebook is designed to obtain the **Image_url** of books from the Goodreads website. The books for which we need to collect this data are those listed in the datasets available [here](https://github.com/zygmuntz/goodbooks-10k). Some of these books have this feature missing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6c4379-8e5e-47fe-9d6e-7146ef891199",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9086083c-def5-4f07-8578-fa893538eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd               # pandas is used for data manipulation and analysis, providing data structures like DataFrames.\n",
    "import numpy as np                # numpy is used for numerical operations on large, multi-dimensional arrays and matrices.\n",
    "import requests                   # Library used for making HTTP requests.\n",
    "from bs4 import BeautifulSoup     # Library for parsing HTML and XML documents.\n",
    "from tqdm import tqdm             # To include a progress bar in the loop.\n",
    "import concurrent.futures         # To make multiple http requests simultaneously.\n",
    "import time                       # time is used for time-related functions\n",
    "import re                         # re provides regular expression matching operations in strings.\n",
    "import json                       # json is used for parsing and generating JSON (JavaScript Object Notation) data.\n",
    "from datetime import datetime     # datetime is used for manipulating dates and times.\n",
    "from IPython.display import Image # IPython's display module to display images within Jupyter Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab905866-4221-4301-9290-1c7eb74aa8a6",
   "metadata": {},
   "source": [
    "### Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31ca3b-38fa-4452-b8ab-9fa3a94dd464",
   "metadata": {},
   "source": [
    "Here we define functions that will be used in a loop to make HTTP requests to Goodreads. This function searches for the cover image url of a book with a given GoodreadsID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4b73cb48-c42d-468e-912a-d0c1bcea9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The header is for the simultaneous requests to work.\n",
    "# It seems that Goodreads blocks the requests made through a script.\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
    "\n",
    "def get_book_editions(book_workid):\n",
    "    url = f\"https://www.goodreads.com/work/editions/{book_workid}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        #print(f\"Error fetching the page: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    editions = soup.find_all('div', class_='editionData')\n",
    "\n",
    "    book_editions = []\n",
    "    for edition in editions:\n",
    "        edition_info = {}\n",
    "        title_tag = edition.find('a', class_='bookTitle')\n",
    "        if title_tag:\n",
    "            edition_info['title'] = title_tag.text.strip()\n",
    "            edition_info['link'] = \"https://www.goodreads.com\" + title_tag['href']\n",
    "    \n",
    "        if edition_info:\n",
    "            book_editions.append(edition_info)\n",
    "\n",
    "    return book_editions\n",
    "\n",
    "\n",
    "def is_image_url_accessible(image_url):\n",
    "    try:\n",
    "        response = requests.get(image_url)\n",
    "        if response.status_code == 200:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_missing_data(edition_url):\n",
    "    response = requests.get(edition_url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        #print(f\"Error fetching the page: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    script_tag = soup.find('script', type='application/ld+json') # The image's url is here\n",
    "\n",
    "    if not script_tag:\n",
    "        print(\"No JSON-LD script found\")\n",
    "        return None\n",
    "\n",
    "    # Convert the JSON fragment into a dictionary\n",
    "    try:\n",
    "        book_details = json.loads(script_tag.string)\n",
    "    except json.JSONDecodeError:\n",
    "        #print(\"Error decoding JSON\")\n",
    "        return None\n",
    "\n",
    "    image_url = book_details.get('image')\n",
    "\n",
    "    if is_image_url_accessible(image_url) == False:\n",
    "        return None\n",
    "\n",
    "    return image_url\n",
    "\n",
    "\n",
    "def get_data(workid):\n",
    "    editions = get_book_editions(workid)\n",
    "    for edition in editions:\n",
    "        image_url = get_missing_data(edition['link'])\n",
    "        if image_url:\n",
    "            return image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6594f2-a3df-4851-a943-8ebc1a587375",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "Here we load the books dataset from which we can get the Goodreads ID of the books for which we want to find the genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d4ca5d-e208-4e8d-a061-3e7f1824688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pd.read_csv(\"../data_preprocessed/books.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94df015-68ef-4b21-bac5-509a228b0042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicates in books[image_url]: 3331\n"
     ]
    }
   ],
   "source": [
    "columns = ['image_url']\n",
    "for column in columns:\n",
    "    num_duplicates = books[books[column].duplicated()].shape[0]\n",
    "    print(f'Number of duplicates in books[{column}]: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6032a9d7-f85b-41d3-a9f0-242795986f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png'} \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://s.gr-assets.com/assets/nophoto/book/111x148-bcc042a9c91a29c1d680899eff700a03.png\" width=\"100\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicated_images = books[books['image_url'].duplicated()]['image_url'].values\n",
    "print(set(duplicated_images), '\\n')\n",
    "# The duplicated images correspond all of them to the same picture and it is actually a missing image:\n",
    "url = duplicated_images[0]\n",
    "Image(url=url, width=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6930f2-6978-40a2-a2d7-122f47239c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_missing_data = books[books['image_url'] == url].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2721a0-5414-49b5-a41e-07851e0dab61",
   "metadata": {},
   "source": [
    "### Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06985e-aa3a-48c0-a9d4-59b62a26ab06",
   "metadata": {},
   "source": [
    "We divide the array of GoodreadsIDs into a number of intervals determined by `divs`. This approach allows us to iterate over these intervals and further within the GoodreadsIDs inside each interval, specifying how many intervals we compile each time. This method gives us control over the web scraping process, as simply iterating over the entire array of GoodreadsIDs, i.e., `books['goodreads_book_id']`, could potentially lead to various issues. Additionally, every time an interval is completed, we store the results in a CSV file to prevent any potential loss of information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a88c776c-c067-410b-baa5-25aa86fddf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:47,  1.11it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:06,  1.78it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:28,  1.35it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:02,  1.91it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:02,  1.92it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:20,  1.49it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<02:48,  1.42s/it]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:07,  1.77it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:38,  1.21it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:49,  1.09it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:01,  1.95it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:26,  1.37it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:01,  1.94it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:43,  1.15it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:41,  1.18it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<02:47,  1.41s/it]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:44,  1.14it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:01,  1.95it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:00,  1.95it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:56,  1.02it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:23,  1.42it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:43,  1.15it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:06,  1.78it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:51,  1.06it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<00:58,  2.03it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:02,  1.90it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:26,  1.37it/s]\n",
      "100.00%|██████████████████████████████████████| 119/119 [00:00<01:39,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "total = len(books_missing_data['work_id']) # total number of ISBNs\n",
    "divs = 28 # number of intervals in which we divide the isbns\n",
    "step = int(total / divs) # length of each interval\n",
    "ranges = [range(step*i - step, step*i) for i in range(1,divs+1)] # an array with the intervals\n",
    "\n",
    "key_i = 0 # number of the interval at which we start the for loop\n",
    "key_f = 28 # number of the interval at which we stop the for loop\n",
    "\n",
    "#progress_bar = tqdm(total=(key_f-key_i), bar_format='{percentage:.2f}%|{bar}| {n_fmt}/{total_fmt} [{remaining}<{elapsed}, {rate_fmt}]')\n",
    "\n",
    "for i in range(key_i, key_f): \n",
    "    \n",
    "    workids = books_missing_data['work_id'][ranges[i]] # WorkIDs of the interval i\n",
    "    \n",
    "    progress_bar = tqdm(total=step, bar_format='{percentage:.2f}%|{bar}| {n_fmt}/{total_fmt} [{remaining}<{elapsed}, {rate_fmt}]')\n",
    "    \n",
    "    data_dict = {\n",
    "        'WorkID':[],\n",
    "        'Image_url':[]\n",
    "    }\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor: \n",
    "        future_to_workid = {executor.submit(get_data, workid): workid for workid in workids}\n",
    "        for future in concurrent.futures.as_completed(future_to_workid):\n",
    "            workid = future_to_workid[future]\n",
    "            try:\n",
    "                image_workid = future.result()\n",
    "                data_dict['WorkID'].append(workid)\n",
    "                data_dict['Image_url'].append(image_workid)\n",
    "            except Exception as e:\n",
    "                data_dict['WorkID'].append(workid)\n",
    "                data_dict['Image_url'].append(np.nan)\n",
    "            progress_bar.update(1)\n",
    "           \n",
    "    progress_bar.close()\n",
    "\n",
    "    new_df = pd.DataFrame(data_dict)\n",
    "\n",
    "    try:\n",
    "        existing_df = pd.read_csv(\"books_image_missing.txt\", sep=\"\\t\")\n",
    "    except FileNotFoundError:\n",
    "        existing_df = pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "\n",
    "    combined_df.to_csv(\"books_image_missing.txt\", sep=\"\\t\", index=False)\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    #progress_bar.update(1) \n",
    "    \n",
    "#progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88a338de-15f9-44d8-893d-a95d383689cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WorkID</th>\n",
       "      <th>Image_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>1180927</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1766737</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>4551869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>2247074</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>3125926</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>1244564</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>25704</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>968512</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>3898716</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>1075398</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>3158544</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>2399497</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>894503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1427</th>\n",
       "      <td>2164503</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>621375</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>1612568</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>2758130</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1902</th>\n",
       "      <td>3077944</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1903</th>\n",
       "      <td>54726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>56119</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>3111256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>25833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2379</th>\n",
       "      <td>2100943</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>21987573</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>3259531</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2498</th>\n",
       "      <td>9295</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2617</th>\n",
       "      <td>13897390</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2736</th>\n",
       "      <td>1254615</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2756</th>\n",
       "      <td>308671</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2855</th>\n",
       "      <td>605176</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>6164528</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>575600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2945</th>\n",
       "      <td>65618</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3045</th>\n",
       "      <td>6642936</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3212</th>\n",
       "      <td>3107820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>2114967</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1629048</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        WorkID Image_url\n",
       "117    1180927       NaN\n",
       "118    1766737       NaN\n",
       "142    4551869       NaN\n",
       "170    2247074       NaN\n",
       "237    3125926       NaN\n",
       "356    1244564       NaN\n",
       "427      25704       NaN\n",
       "713     968512       NaN\n",
       "754    3898716       NaN\n",
       "832    1075398       NaN\n",
       "951    3158544       NaN\n",
       "1070   2399497       NaN\n",
       "1189    894503       NaN\n",
       "1427   2164503       NaN\n",
       "1663    621375       NaN\n",
       "1665   1612568       NaN\n",
       "1784   2758130       NaN\n",
       "1902   3077944       NaN\n",
       "1903     54726       NaN\n",
       "1929     56119       NaN\n",
       "2021   3111256       NaN\n",
       "2022     25833       NaN\n",
       "2379   2100943       NaN\n",
       "2411  21987573       NaN\n",
       "2442   3259531       NaN\n",
       "2498      9295       NaN\n",
       "2617  13897390       NaN\n",
       "2736   1254615       NaN\n",
       "2756    308671       NaN\n",
       "2855    605176       NaN\n",
       "2901   6164528       NaN\n",
       "2903    575600       NaN\n",
       "2945     65618       NaN\n",
       "3045   6642936       NaN\n",
       "3212   3107820       NaN\n",
       "3330   2114967      None\n",
       "3331   1629048      None"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df[combined_df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc7228d-dae4-4a8a-a471-2a2f14b66b6c",
   "metadata": {},
   "source": [
    "The urls that are still missing, are obtained manually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
